{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e85740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np  \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "from abc import ABC, abstractmethod\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff25da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# プレイの様子を動画で見てみるための関数\n",
    "def display_video(frames):\n",
    "    plt.figure(figsize=(8, 8), dpi=50)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
    "    display(HTML(anim.to_jshtml()))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea18ed0d",
   "metadata": {},
   "source": [
    "# CarRacing環境の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca5121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v3') # 環境の作成\n",
    "\n",
    "obs, info = env.reset(seed=0)\n",
    "frames = []\n",
    "total_reward = 0\n",
    "print(obs.shape)  # (96, 96, 3)\n",
    "\n",
    "for _ in range(1000):\n",
    "    frames.append(obs)\n",
    "    action = env.action_space.sample()  # 行動空間から一様ランダムに行動をサンプル\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "    obs = next_obs\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "print('Reward: ', total_reward)\n",
    "display_video(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9a62d1",
   "metadata": {},
   "source": [
    "# 学習に用いるクラスを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c67327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習を効率的に進めるために環境をラップするクラスを定義する\n",
    "# rgbの３チャネルをグレースケールに変換し、4フレーム分をスタックして状態とする\n",
    "# 1ステップの間に6回行動を繰り返し、その合計報酬を1ステップの報酬とする\n",
    "# 直近100ステップの平均報酬が-0.1以下になったらエピソードを終了する\n",
    "# また、コースから大幅に外れた場合もエピソードを終了する\n",
    "\n",
    "class WrappedEnv():\n",
    "    def __init__(self, seed=0):\n",
    "        self.env = gym.make('CarRacing-v3')\n",
    "        self.seed = seed\n",
    "    \n",
    "    def reset(self, visualize=False):\n",
    "        self.visualize = visualize\n",
    "        self.frames = []\n",
    "        self.counter = 0\n",
    "        self.av_r = self.reward_memory()\n",
    "\n",
    "        img_rgb, info = self.env.reset(seed=self.seed)\n",
    "        img_gray = self.rgb2gray(img_rgb)\n",
    "        self.stack = [img_gray, img_gray, img_gray, img_gray]\n",
    "        return np.array(self.stack)\n",
    "    \n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        for _ in range(6):\n",
    "            img_rgb, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            if self.visualize:\n",
    "                self.frames.append(img_rgb)\n",
    "            if np.mean(img_rgb[:, :, 0]) > 185.0: # コース外の色が緑色なので、緑の成分が大きいとコース外とみなす\n",
    "                reward -= 0.05\n",
    "            total_reward += reward\n",
    "            if self.av_r(reward) <= -0.1:\n",
    "                done = True\n",
    "            if done:\n",
    "                break\n",
    "        img_gray = self.rgb2gray(img_rgb)\n",
    "        self.stack.pop(0)\n",
    "        self.stack.append(img_gray)\n",
    "        return np.array(self.stack), total_reward, done, info\n",
    "    \n",
    "    @staticmethod\n",
    "    def rgb2gray(rgb, norm=True):\n",
    "        # rgb image -> gray [0, 1]\n",
    "        gray = np.dot(rgb[..., :], [0.299, 0.587, 0.114])\n",
    "        if norm:\n",
    "            # normalize\n",
    "            gray = gray / 128. - 1.\n",
    "        return gray\n",
    "\n",
    "    @staticmethod\n",
    "    def reward_memory():\n",
    "        # record reward for last 100 steps\n",
    "        count = 0\n",
    "        length = 100\n",
    "        history = np.zeros(length)\n",
    "\n",
    "        def memory(reward):\n",
    "            nonlocal count\n",
    "            history[count] = reward\n",
    "            count = (count + 1) % length\n",
    "            return np.mean(history)\n",
    "\n",
    "        return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f4569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第三回講義資料のものに、early stoppingとbest modelを保存する機能を追加したもの\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, env, algo, seed=0, num_steps=10**6, eval_interval=10**4, num_eval_episodes=3, is_early_stop=True):\n",
    "\n",
    "        self.env = env\n",
    "        self.algo = algo\n",
    "        self.best_return = -float('inf')\n",
    "        self.eval_times_from_best = 0\n",
    "        self.is_early_stop = is_early_stop\n",
    "\n",
    "        # 平均収益を保存するための辞書．\n",
    "        self.returns = {'step': [], 'return': []}\n",
    "\n",
    "        # データ収集を行うステップ数．\n",
    "        self.num_steps = num_steps\n",
    "        # 評価の間のステップ数(インターバル)．\n",
    "        self.eval_interval = eval_interval\n",
    "        # 評価を行うエピソード数．\n",
    "        self.num_eval_episodes = num_eval_episodes\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" num_stepsステップの間，データ収集・学習・評価を繰り返す． \"\"\"\n",
    "\n",
    "        # 学習開始の時間\n",
    "        self.start_time = time()\n",
    "        # エピソードのステップ数．\n",
    "        t = 0\n",
    "\n",
    "        # 環境を初期化する．\n",
    "        state = self.env.reset()\n",
    "\n",
    "        for steps in range(1, self.num_steps + 1):\n",
    "            # 環境(self.env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\n",
    "            # アルゴリズムに渡し，状態・エピソードのステップ数を更新する．\n",
    "            state, t = self.algo.step(self.env, state, t, steps)\n",
    "\n",
    "            # アルゴリズムが準備できていれば，1回学習を行う．\n",
    "            if self.algo.is_update(steps):\n",
    "                self.algo.update()\n",
    "\n",
    "            # 一定のインターバルで評価する．\n",
    "            if steps % self.eval_interval == 0:\n",
    "                early_stop = self.evaluate(steps)\n",
    "                if early_stop:\n",
    "                    break\n",
    "\n",
    "\n",
    "    def evaluate(self, steps):\n",
    "        \"\"\" 複数エピソード環境を動かし，平均収益を記録する． \"\"\"\n",
    "\n",
    "        returns = []\n",
    "        for _ in range(self.num_eval_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            episode_return = 0.0\n",
    "\n",
    "            while (not done):\n",
    "                action = self.algo.exploit(state)\n",
    "                state, reward, done, info = self.env.step(action)\n",
    "                episode_return += reward\n",
    "\n",
    "            returns.append(episode_return)\n",
    "\n",
    "        mean_return = np.mean(returns)\n",
    "        self.returns['step'].append(steps)\n",
    "        self.returns['return'].append(mean_return)\n",
    "\n",
    "        print(f'Num steps: {steps:<6}   '\n",
    "              f'Return: {mean_return:<5.1f}   '\n",
    "              f'Time: {self.time}')\n",
    "        \n",
    "        if mean_return > self.best_return:\n",
    "            self.best_return = mean_return\n",
    "            self.eval_times_from_best = 0\n",
    "            self.algo.save_best_model()\n",
    "            print(f'Best model saved with return: {self.best_return:.1f}')\n",
    "        else:\n",
    "            self.eval_times_from_best += 1\n",
    "        \n",
    "        if self.is_early_stop and self.eval_times_from_best >= 10:\n",
    "            print(\"Early stopping as no improvement in the last 10 evaluations.\")\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def visualize(self):\n",
    "        \"\"\" 1エピソード分動画を再生する． \"\"\"\n",
    "        state = self.env.reset(visualize=True)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while done is False:\n",
    "            action = self.algo.exploit(state)\n",
    "            state, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        print('Reward: ', total_reward)\n",
    "        display_video(self.env.frames)\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\" 平均収益のグラフを描画する． \"\"\"\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        plt.plot(self.returns['step'], self.returns['return'])\n",
    "        plt.xlabel('Steps', fontsize=24)\n",
    "        plt.ylabel('Return', fontsize=24)\n",
    "        plt.tick_params(labelsize=18)\n",
    "        plt.title(f'{self.env.env.unwrapped.spec.id}', fontsize=24)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    def load_best_model(self):\n",
    "        \"\"\" 最良のモデルパラメータを読み込む． \"\"\"\n",
    "        self.algo.load_best_model()\n",
    "        self.best_return = -float('inf')\n",
    "        self.eval_times_from_best = 0\n",
    "        self.returns = {'step': [], 'return': []}\n",
    "\n",
    "    @property\n",
    "    def time(self):\n",
    "        \"\"\" 学習開始からの経過時間． \"\"\"\n",
    "        return str(timedelta(seconds=int(time() - self.start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c2859",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algorithm(ABC):\n",
    "\n",
    "    def explore(self, state):\n",
    "        \"\"\" 確率論的な行動と，その行動の確率密度の対数 \\log(\\pi(a|s)) を返す． \"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze_(0)  \n",
    "        with torch.no_grad():\n",
    "            action, log_pi = self.actor.sample(state)\n",
    "        return action.cpu().numpy()[0], log_pi.item()\n",
    "\n",
    "    def exploit(self, state):\n",
    "        \"\"\" 決定論的な行動を返す． \"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze_(0)  \n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state)\n",
    "        return action.cpu().numpy()[0]\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_update(self, steps):\n",
    "        \"\"\" 現在のトータルのステップ数(steps)を受け取り，アルゴリズムを学習するか否かを返す． \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, env, state, t, steps):\n",
    "        \"\"\" 環境(env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\n",
    "            受け取り，リプレイバッファへの保存などの処理を行い，状態・エピソードのステップ数を更新する．\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        \"\"\" 1回分の学習を行う． \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_best_model(self):\n",
    "        \"\"\" 最良のモデルパラメータを保存する． \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_best_model(self):\n",
    "        \"\"\" 最良のモデルパラメータを読み込む． \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33c5e6",
   "metadata": {},
   "source": [
    "# PPOの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f1ece",
   "metadata": {},
   "source": [
    "## 必要な関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93285ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_pi_tanh(log_stds, noises, actions):\n",
    "    \"\"\" 確率論的な行動の確率密度を返す． \"\"\"\n",
    "    # ガウス分布 `N(0, stds * I)` における `noises * stds` の確率密度の対数(= \\log \\pi(u|a))を計算する．\n",
    "    # (torch.distributions.Normalを使うと無駄な計算が生じるので，下記では直接計算しています．)\n",
    "    gaussian_log_probs = \\\n",
    "        (-0.5 * noises.pow(2) - log_stds).sum(dim=-1, keepdim=True) - 0.5 * math.log(2 * math.pi) * log_stds.size(-1)\n",
    "\n",
    "    # tanh による確率密度の変化を修正する．\n",
    "    log_pis = gaussian_log_probs - torch.log(1 - actions.pow(2) + 1e-6).sum(dim=-1, keepdim=True)\n",
    "\n",
    "    return log_pis\n",
    "\n",
    "def calculate_log_pi_sigmoid(log_stds, noises, actions):\n",
    "    \"\"\" 確率論的な行動の確率密度を返す (sigmoid版). \"\"\"\n",
    "    # ガウス分布の対数確率密度\n",
    "    gaussian_log_probs = \\\n",
    "        (-0.5 * noises.pow(2) - log_stds).sum(dim=-1, keepdim=True) - 0.5 * math.log(2 * math.pi) * log_stds.size(-1)\n",
    "\n",
    "    # sigmoid のヤコビアン補正\n",
    "    log_pis = gaussian_log_probs -torch.log(actions * (1 - actions) + 1e-6).sum(dim=-1, keepdim=True)\n",
    "\n",
    "    return log_pis\n",
    "\n",
    "def reparameterize(means, log_stds):\n",
    "    \"\"\" Reparameterization Trickを用いて，確率論的な行動とその確率密度を返す． \"\"\"\n",
    "    # 標準偏差．\n",
    "    stds = log_stds.exp()\n",
    "    # 標準ガウス分布から，ノイズをサンプリングする．\n",
    "    noises = torch.randn_like(means)\n",
    "    # Reparameterization Trickを用いて，N(means, stds)からのサンプルを計算する．\n",
    "    us = means + noises * stds\n",
    "    # tanh　を適用し，確率論的な行動を計算する．\n",
    "    a0 = torch.tanh(us[:, 0:1])   # shape (B, 1)  <- preserves batch dim\n",
    "    a12 = torch.sigmoid(us[:, 1:3])# shape (B, 2)\n",
    "    actions = torch.cat([a0, a12], dim=1)  # shape (B, 3)\n",
    "\n",
    "    # 確率論的な行動の確率密度の対数を計算する．\n",
    "    log_pis_tanh = calculate_log_pi_tanh(log_stds[:, 0:1], noises[:, 0:1], a0)\n",
    "    log_pis_sigmoid = calculate_log_pi_sigmoid(log_stds[:, 1:3], noises[:, 1:3], a12)\n",
    "    log_pis = log_pis_tanh + log_pis_sigmoid\n",
    "\n",
    "    return actions, log_pis\n",
    "\n",
    "def atanh(x):\n",
    "    \"\"\" tanh の逆関数． \"\"\"\n",
    "    return 0.5 * (torch.log(1 + x + 1e-6) - torch.log(1 - x + 1e-6))\n",
    "\n",
    "def logit(x):\n",
    "    \"\"\" sigmoid の逆関数 \"\"\"\n",
    "    # x が 0 または 1 にならないように eps を加える\n",
    "    eps = 1e-6\n",
    "    x = x.clamp(min=eps, max=1-eps)\n",
    "    return torch.log(x / (1 - x))\n",
    "\n",
    "def evaluate_log_pi(means, log_stds, actions):\n",
    "    \"\"\" 平均(mean)，標準偏差の対数(log_stds)でパラメータ化した方策における，行動(actions)の確率密度の対数を計算する． \"\"\"\n",
    "    stds = log_stds.exp()\n",
    "    noises0 = (atanh(actions[:, 0:1]) - means[:, 0:1]) / (stds[:, 0:1] + 1e-8)\n",
    "    noises12 = (logit(actions[:, 1:3]) - means[:, 1:3]) / (stds[:, 1:3] + 1e-8)\n",
    "    return calculate_log_pi_tanh(log_stds[:, 0:1], noises0, actions[:, 0:1]) + \\\n",
    "           calculate_log_pi_sigmoid(log_stds[:, 1:3], noises12, actions[:, 1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b525b9",
   "metadata": {},
   "source": [
    "## nnの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47487048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CarRacingEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),  # -> (32, 23, 23)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # -> (64, 10, 10)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), # -> (64, 8, 8)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 512),  # 4096 → 512\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x  # shape: (batch, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5124b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOActor(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 3)\n",
    "        )\n",
    "        self.log_stds = nn.Parameter(torch.zeros(1, 3))\n",
    "\n",
    "    def forward(self, states):\n",
    "        h = self.encoder(states)\n",
    "        action = self.head(h)\n",
    "        a0 = torch.tanh(action[:, 0:1])   # shape (B, 1)  <- preserves batch dim\n",
    "        a12 = torch.sigmoid(action[:, 1:3])  # shape (B, 2)\n",
    "        action = torch.cat([a0, a12], dim=1)  # shape (B, 3)\n",
    "        return action\n",
    "\n",
    "    def sample(self, states):\n",
    "        h = self.encoder(states)\n",
    "        means = self.head(h)\n",
    "        return reparameterize(means, self.log_stds)\n",
    "\n",
    "    def evaluate_log_pi(self, states, actions):\n",
    "        h = self.encoder(states)\n",
    "        means = self.head(h)\n",
    "        return evaluate_log_pi(means, self.log_stds, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32248017",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(512, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, states):\n",
    "        h = self.encoder(states)\n",
    "        return self.net(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0189fb",
   "metadata": {},
   "source": [
    "## GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceb8044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advantage(values, rewards, dones, next_values, gamma=0.995, lambd=0.997):\n",
    "    \"\"\" GAEを用いて，状態価値のターゲットとGAEを計算する． \"\"\"\n",
    "\n",
    "    # TD誤差を計算する．\n",
    "    deltas = rewards + gamma * next_values * (1 - dones) - values\n",
    "\n",
    "    # GAEを初期化する．\n",
    "    advantages = torch.empty_like(rewards)\n",
    "\n",
    "    # 終端ステップを計算する．\n",
    "    advantages[-1] = deltas[-1]\n",
    "\n",
    "    # 終端ステップの1つ前から，順番にGAEを計算していく．\n",
    "    for t in reversed(range(rewards.size(0) - 1)):\n",
    "        advantages[t] = deltas[t] + gamma * lambd * (1 - dones[t]) * advantages[t + 1]\n",
    "\n",
    "    # 状態価値のターゲットをλ-収益として計算する．\n",
    "    targets = advantages + values\n",
    "\n",
    "    # GAEを標準化する．\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    return targets, advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35638a4d",
   "metadata": {},
   "source": [
    "## 学習アルゴリズム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3286ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "\n",
    "    def __init__(self, buffer_size, state_shape, action_shape, device=torch.device('cuda')):\n",
    "        storage_shape = (buffer_size, *state_shape)\n",
    "        self.states = torch.empty(storage_shape, dtype=torch.float, device=device)\n",
    "        self.actions = torch.empty((buffer_size, *action_shape), dtype=torch.float, device=device)\n",
    "        self.rewards = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.dones = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.log_pis = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.next_states = torch.empty(storage_shape, dtype=torch.float, device=device)\n",
    "\n",
    "        # 次にデータを挿入するインデックス．\n",
    "        self._p = 0\n",
    "        # バッファのサイズ．\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def append(self, state, action, reward, done, log_pi, next_state):\n",
    "        # state, next_state: numpy in HWC -> convert to CHW tensor on buffer device\n",
    "        dev = self.states.device\n",
    "        self.states[self._p].copy_(torch.from_numpy(state).float().to(dev))\n",
    "        self.actions[self._p].copy_(torch.from_numpy(action).float().to(dev))\n",
    "        self.rewards[self._p] = float(reward)\n",
    "        self.dones[self._p] = float(done)\n",
    "        self.log_pis[self._p] = float(log_pi)\n",
    "        self.next_states[self._p].copy_(torch.from_numpy(next_state).float().to(dev))\n",
    "        self._p = (self._p + 1) % self.buffer_size\n",
    "\n",
    "    def get(self):\n",
    "        assert self._p == 0, 'Buffer needs to be full before training.'\n",
    "        return self.states, self.actions, self.rewards, self.dones, self.log_pis, self.next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1be023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_updatesを15としているが、10などもっと小さくした方が、clippingがききやすく安定するはず。\n",
    "\n",
    "class PPO(Algorithm):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape, device=torch.device('cuda'), seed=0,\n",
    "                 batch_size=256, gamma=0.995, lr_actor=3e-4, lr_critic=3e-4,\n",
    "                 rollout_length=2048, num_updates=15, clip_eps=0.2, lambd=0.97,\n",
    "                 coef_ent=0.0, max_grad_norm=0.5, actor_class=PPOActor, critic_class=PPOCritic,\n",
    "                 encoder_class=CarRacingEncoder, best_model_path='ppo_best_model.pth'):\n",
    "        super().__init__()\n",
    "\n",
    "        # シードを設定する．\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "        # データ保存用のバッファ．\n",
    "        self.buffer = RolloutBuffer(\n",
    "            buffer_size=rollout_length,\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Actor-Criticのネットワークを構築する．\n",
    "        a_encoder = encoder_class().to(device)\n",
    "        c_encoder = encoder_class().to(device)\n",
    "        self.actor = actor_class(a_encoder).to(device)\n",
    "        self.critic = critic_class(c_encoder).to(device)\n",
    "\n",
    "        # オプティマイザ．\n",
    "        self.optim_actor = torch.optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.optim_critic = torch.optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "\n",
    "        # その他パラメータ．\n",
    "        self.learning_steps = 0\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.rollout_length = rollout_length\n",
    "        self.num_updates = num_updates\n",
    "        self.clip_eps = clip_eps\n",
    "        self.lambd = lambd\n",
    "        self.coef_ent = coef_ent\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.best_model_path = best_model_path\n",
    "\n",
    "    def is_update(self, steps):\n",
    "        # ロールアウト1回分のデータが溜まったら学習する．\n",
    "        return steps % self.rollout_length == 0\n",
    "\n",
    "    def step(self, env, state, t, steps):\n",
    "        t += 1\n",
    "\n",
    "        action, log_pi = self.explore(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # バッファにデータを追加する．\n",
    "        self.buffer.append(state, action, reward, done, log_pi, next_state)\n",
    "\n",
    "        # エピソードが終了した場合には，環境をリセットする．\n",
    "        if done:\n",
    "            t = 0\n",
    "            next_state = env.reset()\n",
    "\n",
    "        return next_state, t\n",
    "\n",
    "    def update(self):\n",
    "        self.learning_steps += 1\n",
    "\n",
    "        states, actions, rewards, dones, log_pis, next_states = self.buffer.get()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            values = self.critic(states)\n",
    "            next_values = self.critic(next_states)\n",
    "        targets, advantages = calculate_advantage(values, rewards, dones, next_values, self.gamma, self.lambd)\n",
    "\n",
    "        # バッファ内のデータを num_updates回ずつ使って，ネットワークを更新する．\n",
    "        for _ in range(self.num_updates):\n",
    "            # インデックスをシャッフルする．\n",
    "            indices = np.arange(self.rollout_length)\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            # ミニバッチに分けて学習する．\n",
    "            for start in range(0, self.rollout_length, self.batch_size):\n",
    "                idxes = indices[start:start+self.batch_size]\n",
    "                self.update_critic(states[idxes], targets[idxes])\n",
    "                self.update_actor(states[idxes], actions[idxes], log_pis[idxes], advantages[idxes])\n",
    "\n",
    "    def update_critic(self, states, targets):\n",
    "        loss_critic = (self.critic(states) - targets).pow_(2).mean()\n",
    "\n",
    "        self.optim_critic.zero_grad()\n",
    "        loss_critic.backward(retain_graph=False)\n",
    "        # 学習を安定させるヒューリスティックとして，勾配のノルムをクリッピングする．\n",
    "        nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
    "        self.optim_critic.step()\n",
    "\n",
    "    def update_actor(self, states, actions, log_pis_old, advantages):\n",
    "        log_pis = self.actor.evaluate_log_pi(states, actions)\n",
    "        mean_entropy = -log_pis.mean()\n",
    "\n",
    "        ratios = (log_pis - log_pis_old).exp_()\n",
    "        loss_actor1 = -ratios * advantages\n",
    "        loss_actor2 = -torch.clamp(\n",
    "            ratios,\n",
    "            1.0 - self.clip_eps,\n",
    "            1.0 + self.clip_eps\n",
    "        ) * advantages\n",
    "        loss_actor = torch.max(loss_actor1, loss_actor2).mean() - self.coef_ent * mean_entropy\n",
    "\n",
    "        self.optim_actor.zero_grad()\n",
    "        loss_actor.backward(retain_graph=False)\n",
    "        # 学習を安定させるヒューリスティックとして，勾配のノルムをクリッピングする．\n",
    "        nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
    "        self.optim_actor.step()\n",
    "\n",
    "    def save_best_model(self):\n",
    "        torch.save({\n",
    "            'actor_state_dict': self.actor.state_dict(),\n",
    "            'critic_state_dict': self.critic.state_dict(),\n",
    "            'optim_actor_state_dict': self.optim_actor.state_dict(),\n",
    "            'optim_critic_state_dict': self.optim_critic.state_dict(),\n",
    "        }, self.best_model_path)\n",
    "\n",
    "    def load_best_model(self):\n",
    "        checkpoint = torch.load(self.best_model_path, map_location=self.device)\n",
    "        self.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "        self.optim_actor.load_state_dict(checkpoint['optim_actor_state_dict'])\n",
    "        self.optim_critic.load_state_dict(checkpoint['optim_critic_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dee288f",
   "metadata": {},
   "source": [
    "## 学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad5d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ID = 'CarRacing-v3'\n",
    "SEED = 0\n",
    "ROLLOUT_LENGTH = 2048\n",
    "NUM_STEPS = 500 * ROLLOUT_LENGTH\n",
    "EVAL_INTERVAL = 5 * ROLLOUT_LENGTH\n",
    "\n",
    "env = WrappedEnv()\n",
    "state_example = env.reset()\n",
    "\n",
    "algo = PPO(\n",
    "    state_shape=state_example.shape,\n",
    "    action_shape=env.env.action_space.shape,\n",
    "    seed=SEED,\n",
    "    device=torch.device(\"cpu\"),\n",
    "    rollout_length=ROLLOUT_LENGTH,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    env=env,\n",
    "    algo=algo,\n",
    "    seed=SEED,\n",
    "    num_steps=NUM_STEPS,\n",
    "    eval_interval=EVAL_INTERVAL,\n",
    "    is_early_stop=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff0d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPUで学習する場合，使用するスレッド数を設定する．\n",
    "# ryzen9 7950x 16-core 32-threads を使用した.\n",
    "torch.set_num_threads(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.plot()\n",
    "trainer.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_best_model()\n",
    "trainer.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f27e198",
   "metadata": {},
   "source": [
    "# actorの構成を変更し、性能を比較する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b28a4f",
   "metadata": {},
   "source": [
    "## ガウス分布からベータ分布に変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29da2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Beta\n",
    "\n",
    "class PPOActorBeta(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 6),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, states):\n",
    "        h = self.encoder(states)\n",
    "        action = self.head(h)\n",
    "        alpha = action[:, 0:3] + 1.0\n",
    "        beta = action[:, 3:6] + 1.0\n",
    "        action = alpha / (alpha + beta)  # expectation of Beta distribution\n",
    "        action = action * torch.tensor([2., 1., 1.], device=action.device) + torch.tensor([-1., 0., 0.], device=action.device)\n",
    "        return action\n",
    "\n",
    "    def sample(self, states):\n",
    "        h = self.encoder(states)\n",
    "        action = self.head(h)\n",
    "        alpha = action[:, 0:3] + 1.0\n",
    "        beta = action[:, 3:6] + 1.0\n",
    "        dist = Beta(alpha, beta)\n",
    "        action = dist.sample()\n",
    "        log_pis = dist.log_prob(action).sum(dim=-1, keepdim=True)\n",
    "        action = action * torch.tensor([2., 1., 1.], device=action.device) + torch.tensor([-1., 0., 0.], device=action.device)\n",
    "        return action, log_pis\n",
    "\n",
    "    def evaluate_log_pi(self, states, actions):\n",
    "        h = self.encoder(states)\n",
    "        action = self.head(h)\n",
    "        alpha = action[:, 0:3] + 1.0\n",
    "        beta = action[:, 3:6] + 1.0\n",
    "        dist = Beta(alpha, beta)\n",
    "        actions = (actions - torch.tensor([-1., 0., 0.], device=actions.device)) / torch.tensor([2., 1., 1.], device=actions.device)\n",
    "        log_pis = dist.log_prob(actions).sum(dim=-1, keepdim=True)\n",
    "        return log_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4658bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ID = 'CarRacing-v3'\n",
    "SEED = 0\n",
    "ROLLOUT_LENGTH = 2048\n",
    "NUM_STEPS = 500 * ROLLOUT_LENGTH\n",
    "EVAL_INTERVAL = 5 * ROLLOUT_LENGTH\n",
    "\n",
    "env = WrappedEnv()\n",
    "state_example = env.reset()\n",
    "\n",
    "algo = PPO(\n",
    "    state_shape=state_example.shape,\n",
    "    action_shape=env.env.action_space.shape,\n",
    "    seed=SEED,\n",
    "    device=torch.device(\"cpu\"),\n",
    "    rollout_length=ROLLOUT_LENGTH,\n",
    "    actor_class=PPOActorBeta,\n",
    "    best_model_path='ppo_beta_best_model.pth'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    env=env,\n",
    "    algo=algo,\n",
    "    seed=SEED,\n",
    "    num_steps=NUM_STEPS,\n",
    "    eval_interval=EVAL_INTERVAL,\n",
    "    is_early_stop=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d88e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.plot()\n",
    "trainer.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1cccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_best_model()\n",
    "trainer.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb47cb00",
   "metadata": {},
   "source": [
    "## ガウス分布の標準偏差もnnの出力に変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de79c6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOActorGaussian2(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, states):\n",
    "        h = self.encoder(states)\n",
    "        action = self.head(h)\n",
    "        a0 = torch.tanh(action[:, 0:1])\n",
    "        a12 = torch.sigmoid(action[:, 1:3])\n",
    "        action = torch.cat([a0, a12], dim=1)\n",
    "        return action\n",
    "\n",
    "    def sample(self, states):\n",
    "        h = self.encoder(states)\n",
    "        actions = self.head(h)\n",
    "        means = actions[:, 0:3]\n",
    "        log_stds = -F.softplus(actions[:, 3:6])  # log_stdsの範囲を０以下に制限\n",
    "        return reparameterize(means, log_stds)\n",
    "\n",
    "    def evaluate_log_pi(self, states, old_actions):\n",
    "        h = self.encoder(states)\n",
    "        actions = self.head(h)\n",
    "        means = actions[:, 0:3]\n",
    "        log_stds = -F.softplus(actions[:, 3:6])  # log_stdsの範囲を０以下に制限\n",
    "        return evaluate_log_pi(means, log_stds, old_actions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80104ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ID = 'CarRacing-v3'\n",
    "SEED = 0\n",
    "ROLLOUT_LENGTH = 2048\n",
    "NUM_STEPS = 500 * ROLLOUT_LENGTH\n",
    "EVAL_INTERVAL = 5 * ROLLOUT_LENGTH\n",
    "\n",
    "env = WrappedEnv()\n",
    "state_example = env.reset()\n",
    "\n",
    "algo = PPO(\n",
    "    state_shape=state_example.shape,\n",
    "    action_shape=env.env.action_space.shape,\n",
    "    seed=SEED,\n",
    "    device=torch.device(\"cpu\"),\n",
    "    rollout_length=ROLLOUT_LENGTH,\n",
    "    actor_class=PPOActorGaussian2,\n",
    "    best_model_path='ppo_gaussian2_best_model.pth'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    env=env,\n",
    "    algo=algo,\n",
    "    seed=SEED,\n",
    "    num_steps=NUM_STEPS,\n",
    "    eval_interval=EVAL_INTERVAL,\n",
    "    is_early_stop=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1025f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.plot()\n",
    "trainer.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_best_model()\n",
    "trainer.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8e071",
   "metadata": {},
   "source": [
    "## ハンドル操作をガウス分布（標準偏差はnnと独立した学習可能パラメータ）、アクセルとブレーキをベータ分布に変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize_for_tanh(means, log_stds):\n",
    "    stds = log_stds.exp()\n",
    "    noises = torch.randn_like(means)\n",
    "    us = means + noises * stds\n",
    "    actions = torch.tanh(us)\n",
    "\n",
    "    log_pis = calculate_log_pi_tanh(log_stds, noises, actions)\n",
    "\n",
    "    return actions, log_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc85f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOActorGausPlusBeta(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 5)\n",
    "        )\n",
    "        self.log_stds = nn.Parameter(torch.zeros(1, 1))\n",
    "\n",
    "    def forward(self, states):\n",
    "        h = self.encoder(states)\n",
    "        action = self.head(h)\n",
    "        mean = action[:, 0:1]\n",
    "        alpha = F.softplus(action[:, 1:3]) + 1.0\n",
    "        beta = F.softplus(action[:, 3:5]) + 1.0\n",
    "        beta_action = alpha / (alpha + beta)  # expectation of Beta distribution\n",
    "        action = torch.cat([torch.tanh(mean), beta_action], dim=1)\n",
    "        return action\n",
    "\n",
    "    def sample(self, states):\n",
    "        h = self.encoder(states)\n",
    "        action = self.head(h)\n",
    "        mean = action[:, 0:1]\n",
    "        alpha = F.softplus(action[:, 1:3]) + 1.0\n",
    "        beta = F.softplus(action[:, 3:5]) + 1.0\n",
    "        dist = Beta(alpha, beta)\n",
    "        beta_action = dist.sample()\n",
    "        beta_log_pis = dist.log_prob(beta_action).sum(dim=-1, keepdim=True)\n",
    "        gaus_action, gaus_log_pis = reparameterize_for_tanh(mean, self.log_stds)\n",
    "        log_pis = beta_log_pis + gaus_log_pis\n",
    "        action = torch.cat([gaus_action, beta_action], dim=1)\n",
    "        return action, log_pis\n",
    "\n",
    "    def evaluate_log_pi(self, states, old_actions):\n",
    "        h = self.encoder(states)\n",
    "        action = self.head(h)\n",
    "        mean = action[:, 0:1]\n",
    "        alpha = F.softplus(action[:, 1:3]) + 1.0\n",
    "        beta = F.softplus(action[:, 3:5]) + 1.0\n",
    "        dist = Beta(alpha, beta)\n",
    "        beta_log_pis = dist.log_prob(old_actions[:, 1:3]).sum(dim=-1, keepdim=True)\n",
    "        stds = self.log_stds.exp()\n",
    "        noises = (atanh(old_actions[:, 0:1]) - mean) / (stds + 1e-8)\n",
    "        gaus_log_pis = calculate_log_pi_tanh(self.log_stds, noises, old_actions[:, 0:1])\n",
    "        return beta_log_pis + gaus_log_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01fb1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ID = 'CarRacing-v3'\n",
    "SEED = 0\n",
    "ROLLOUT_LENGTH = 2048\n",
    "NUM_STEPS = 500 * ROLLOUT_LENGTH\n",
    "EVAL_INTERVAL = 5 * ROLLOUT_LENGTH\n",
    "\n",
    "env = WrappedEnv()\n",
    "state_example = env.reset()\n",
    "\n",
    "algo = PPO(\n",
    "    state_shape=state_example.shape,\n",
    "    action_shape=env.env.action_space.shape,\n",
    "    seed=SEED,\n",
    "    device=torch.device(\"cpu\"),\n",
    "    rollout_length=ROLLOUT_LENGTH,\n",
    "    actor_class=PPOActorGausPlusBeta,\n",
    "    best_model_path='ppo_gaus_plus_beta_best_model.pth'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    env=env,\n",
    "    algo=algo,\n",
    "    seed=SEED,\n",
    "    num_steps=NUM_STEPS,\n",
    "    eval_interval=EVAL_INTERVAL,\n",
    "    is_early_stop=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a03a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.plot()\n",
    "trainer.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea17959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_best_model()\n",
    "trainer.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537e0373",
   "metadata": {},
   "source": [
    "## ハンドル操作をガウス分布（標準偏差もnnの出力を利用）、アクセルとブレーキをベータ分布に変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc6f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOActorGausPlusBeta2(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, states):\n",
    "        h = self.encoder(states)\n",
    "        action = self.head(h)\n",
    "        mean = action[:, 0:1]\n",
    "        alpha = F.softplus(action[:, 1:3]) + 1.0\n",
    "        beta = F.softplus(action[:, 3:5]) + 1.0\n",
    "        beta_action = alpha / (alpha + beta)  # expectation of Beta distribution\n",
    "        action = torch.cat([torch.tanh(mean), beta_action], dim=1)\n",
    "        return action\n",
    "\n",
    "    def sample(self, states):\n",
    "        h = self.encoder(states)\n",
    "        action = self.head(h)\n",
    "        mean = action[:, 0:1]\n",
    "        alpha = F.softplus(action[:, 1:3]) + 1.0\n",
    "        beta = F.softplus(action[:, 3:5]) + 1.0\n",
    "        log_stds = -F.softplus(action[:, 5:6])  # log_stdsの範囲を０以下に制限  \n",
    "        dist = Beta(alpha, beta)\n",
    "        beta_action = dist.sample()\n",
    "        beta_log_pis = dist.log_prob(beta_action).sum(dim=-1, keepdim=True)\n",
    "        gaus_action, gaus_log_pis = reparameterize_for_tanh(mean, log_stds)\n",
    "        log_pis = beta_log_pis + gaus_log_pis\n",
    "        action = torch.cat([gaus_action, beta_action], dim=1)\n",
    "        return action, log_pis\n",
    "\n",
    "    def evaluate_log_pi(self, states, old_actions):\n",
    "        h = self.encoder(states)\n",
    "        action = self.head(h)\n",
    "        mean = action[:, 0:1]\n",
    "        alpha = F.softplus(action[:, 1:3]) + 1.0\n",
    "        beta = F.softplus(action[:, 3:5]) + 1.0\n",
    "        log_stds = -F.softplus(action[:, 5:6])  # log_stdsの範囲を０以下に制限\n",
    "        dist = Beta(alpha, beta)\n",
    "        beta_log_pis = dist.log_prob(old_actions[:, 1:3]).sum(dim=-1, keepdim=True)\n",
    "        stds = log_stds.exp()\n",
    "        noises = (atanh(old_actions[:, 0:1]) - mean) / (stds + 1e-8)\n",
    "        gaus_log_pis = calculate_log_pi_tanh(log_stds, noises, old_actions[:, 0:1])\n",
    "        return beta_log_pis + gaus_log_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f541ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ID = 'CarRacing-v3'\n",
    "SEED = 0\n",
    "ROLLOUT_LENGTH = 2048\n",
    "NUM_STEPS = 500 * ROLLOUT_LENGTH\n",
    "EVAL_INTERVAL = 5 * ROLLOUT_LENGTH\n",
    "\n",
    "env = WrappedEnv()\n",
    "state_example = env.reset()\n",
    "\n",
    "algo = PPO(\n",
    "    state_shape=state_example.shape,\n",
    "    action_shape=env.env.action_space.shape,\n",
    "    seed=SEED,\n",
    "    device=torch.device(\"cpu\"),\n",
    "    rollout_length=ROLLOUT_LENGTH,\n",
    "    actor_class=PPOActorGausPlusBeta2,\n",
    "    best_model_path='ppo_gaus_plus_beta2_best_model.pth'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    env=env,\n",
    "    algo=algo,\n",
    "    seed=SEED,\n",
    "    num_steps=NUM_STEPS,\n",
    "    eval_interval=EVAL_INTERVAL,\n",
    "    is_early_stop=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57489888",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.plot()\n",
    "trainer.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3470f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_best_model()\n",
    "trainer.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e2d578",
   "metadata": {},
   "source": [
    "# 参考文献\n",
    "pytorch_car_caring:\n",
    "https://github.com/xtma/pytorch_car_caring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
